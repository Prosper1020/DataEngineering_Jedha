{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project ETL part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to process two datasets, transform and merge them to make a report to the warehouse operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.amazonaws.auth.{AWSStaticCredentialsProvider, BasicAWSCredentials}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val awsAccessKeyId = \"**********\"\n",
    "val awsSecretKey = \"***********\"\n",
    "val kinesisStreamName = \"Jedha-pickers\"\n",
    "val kinesisRegion = \"eu-west-1\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the aws environement variables on the spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.hadoopConfiguration.set(\"fs.s3a.access.key\", awsAccessKeyId)\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.secret.key\", awsSecretKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the created by Firehose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var orders = spark.read.json(\"s3://path/defined/in/firehose/*/*/*/*/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse the date as a timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = orders.withColumn(\"timestamp\", date_format($\"timestamp\",\"yyyy-MM-dd HH-mm-ss.mm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each order we only select the latest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val w = Window.partitionBy($\"order_id\").orderBy($\"timestamp\".desc)\n",
    "val only_latest_orders = orders.withColumn(\"rank\", row_number().over(w)).where($\"rank\" === 1).drop(\"rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very condensed:\n",
    "- On the first line we parse the array of items\n",
    "- One the 2nd line we extract the block form the location string\n",
    "- On the 3d line we select the nested sku\n",
    "- Then we remove the temporary column \"block\" Groupy by block, and count the items for each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val itemsPerBlock = only_latest_orders\n",
    "      .select($\"order_id\", $\"status\", $\"timestamp\", explode($\"items\").as(\"item\"))\n",
    "      .withColumn(\"block\", regexp_extract($\"item.location\", \"BLOCK-(.+)-(.+)\", 2))\n",
    "      .withColumn(\"sku\", $\"item.sku\")\n",
    "      .drop(\"item\")\n",
    "      .groupBy($\"block\")\n",
    "      .count()\n",
    "      .as(\"items_to_pick\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use spark streaming to get the pickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val kinesis = spark.readStream\n",
    "  .format(\"kinesis\")\n",
    "  .option(\"streamName\", kinesisStreamName)\n",
    "  .option(\"region\", kinesisRegion)\n",
    "  .option(\"initialPosition\", \"TRIM_HORIZON\")\n",
    "  .option(\"checkpointLocation\", \"s3://output-databricks/checkpoint/\")\n",
    "  .option(\"awsAccessKey\", awsAccessKeyId)\n",
    "  .option(\"awsSecretKey\", awsSecretKey)\n",
    "  .load()\n",
    "\n",
    "\n",
    "val schema = StructType(Seq(\n",
    "  StructField(\"block\", StringType),\n",
    "  StructField(\"action\", StringType),\n",
    "))\n",
    "\n",
    "val result = kinesis.selectExpr(\"CAST(data as STRING) as json\")\n",
    "  .withColumn(\"picker\", from_json($\"json\", schema))\n",
    "  .withColumn(\"block\", expr(\"picker.block\"))\n",
    "  .withColumn(\"action\", expr(\"picker.action\"))\n",
    "  .withColumn(\"picker_delta\", when(($\"action\" === \"add\"), 1).otherwise(-1))\n",
    "  .drop(\"json\")\n",
    "  .drop(\"action\")\n",
    "  .drop(\"order\")\n",
    "  .groupBy(\"block\").agg(sum($\"picker_delta\").as(\"pickers\"))\n",
    "  .withColumn(\"items_per_hour\", $\"pickers\" * 10)\n",
    "  .drop(\"pickers\")\n",
    "  .join(itemsPerBlock, \"block\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice to have on databricks: \n",
    "you can make a plot to visualize the blocks. \n",
    "Use:\n",
    "- **keys**: `block`\n",
    "- **values**: `count`, `items_per_hour`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
