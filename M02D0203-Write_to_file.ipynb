{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercices we reuse the code from the previous exercice but instead of grouping by, we will simply save the resulting dataset as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.amazonaws.services.kinesis.model.PutRecordRequest\n",
    "import com.amazonaws.services.kinesis.AmazonKinesisClientBuilder\n",
    "import com.amazonaws.auth.{AWSStaticCredentialsProvider, BasicAWSCredentials}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import collection.JavaConverters._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val awsAccessKeyId = \"YOUR ACCESS KEY ID\"\n",
    "val awsSecretKey = \"YOUR SECRET KEY\"\n",
    "val kinesisStreamName = \"YOUR STREAM NAME\"\n",
    "val kinesisRegion = \"YOUR REGION\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.hadoopConfiguration.set(\"fs.s3a.access.key\", awsAccessKeyId)\n",
    "sc.hadoopConfiguration.set(\"fs.s3a.secret.key\", awsSecretKey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Implement a spark streming listener that can save to a csv on s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val kinesis = spark.readStream\n",
    "  .format(\"kinesis\")\n",
    "  .option(\"streamName\", kinesisStreamName)\n",
    "  .option(\"region\", kinesisRegion)\n",
    "  .option(\"initialPosition\", \"TRIM_HORIZON\")\n",
    "  .option(\"checkpointLocation\", \"s3://output-databricks/checkpoint/\")\n",
    "  .option(\"awsAccessKey\", awsAccessKeyId)\n",
    "  .option(\"awsSecretKey\", awsSecretKey)\n",
    "  .load()\n",
    "\n",
    "\n",
    "val schema = StructType(Seq(\n",
    "  StructField(\"items\", ArrayType(StringType)),\n",
    "  StructField(\"from\", StringType),\n",
    "  StructField(\"to\", StringType),\n",
    "))\n",
    "\n",
    "val result = kinesis.selectExpr(\"CAST(data as STRING) as json\")\n",
    "  .withColumn(\"order\", from_json($\"json\", schema))\n",
    "  .withColumn(\"item_count\", size(expr(\"order.items\")))\n",
    "  .withColumn(\"from\", expr(\"order.from\"))\n",
    "  .withColumn(\"to\", expr(\"order.to\"))\n",
    "  .drop(\"json\")\n",
    "  .drop(\"order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using display to trigger the stream, we use `writeStream`. This will write the stream to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query = result.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"csv\")\n",
    "    .option(\"path\", \"s3://output-databricks/\")\n",
    "    .option(\"checkpointLocation\", \"s3://output-databricks/checkpoint/\")\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper code to publish the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "import java.lang.reflect.{Type, ParameterizedType}\n",
    "import com.fasterxml.jackson.databind.ObjectMapper\n",
    "import com.fasterxml.jackson.module.scala.DefaultScalaModule\n",
    "import com.fasterxml.jackson.annotation.JsonProperty;\n",
    "import com.fasterxml.jackson.core.`type`.TypeReference;\n",
    "import java.nio.ByteBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Order(from: String, to: String, items: Seq[String])\n",
    "\n",
    "def getRandomElement[A](seq: Seq[A]): A = {\n",
    "  val r = scala.util.Random\n",
    "  seq(r.nextInt(seq.length))\n",
    "}\n",
    "    \n",
    "def selectNElementFromList[A](maxElements: Int, list: Seq[A]): Seq[A] = {\n",
    "  val r = scala.util.Random\n",
    "  (0 until r.nextInt(maxElements) + 1).map(idx => list(r.nextInt(list.length))).toSeq\n",
    "}\n",
    "\n",
    "val kinesisClient = AmazonKinesisClientBuilder.standard()\n",
    "  .withRegion(kinesisRegion)\n",
    "  .withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(awsAccessKeyId, awsSecretKey)))\n",
    "  .build()\n",
    "\n",
    "println(s\"Putting words onto stream $kinesisStreamName\")\n",
    "var lastSequenceNumber: String = null\n",
    "\n",
    "val SOURCES = Seq(\"Paris\", \"Lyon\", \"Marseille\", \"Bordeau\")\n",
    "val DESTINATIONS = Seq(\"Berlin\", \"Madrid\", \"Rome\", \"London\")\n",
    "val ITEMS = Seq(\"Adidas Kampung\",\"Ballet shoe\",\"Pointe shoe\",\"Bast shoe\",\"Blucher shoe\",\"Boat shoe\",\"Brogan\",\"Brogue shoe\",\"Brothel creeper\",\"Bucks\",\"Cantabrian albarcas\",\"Chelsea boot\",\"Chopine\",\"Chukka boot\",\"Climbing shoe\",\"Clog\",\"Court shoe\",\"Cross country running shoe\",\"Derby shoe\",\"Desert Boot\",\"Diabetic shoe\",\"Dress shoe\",\"Duckbill shoe\",\"Driving moccasins\",\"Earth shoe\",\"Elevator shoes\",\"Espadrille\",\"Fashion boot\",\"Galesh\",\"Geta\",\"Giveh\",\"High-heeled\")\n",
    "\n",
    "val mapper = new ObjectMapper()\n",
    "mapper.registerModule(DefaultScalaModule)\n",
    "\n",
    "for (i <- 0 to 10) {\n",
    "  val time = System.currentTimeMillis\n",
    "  \n",
    "  val data = mapper.writeValueAsString(Order(getRandomElement(SOURCES), getRandomElement(DESTINATIONS), selectNElementFromList(5, ITEMS)))\n",
    "  val request = new PutRecordRequest()\n",
    "      .withStreamName(kinesisStreamName)\n",
    "      .withPartitionKey(\"some-key\")\n",
    "      .withData(ByteBuffer.wrap(data.getBytes()))\n",
    "  if (lastSequenceNumber != null) {\n",
    "    request.setSequenceNumberForOrdering(lastSequenceNumber)\n",
    "  }    \n",
    "  val result = kinesisClient.putRecord(request)\n",
    "  lastSequenceNumber = result.getSequenceNumber()\n",
    "\n",
    "  Thread.sleep(math.max(10000 - (System.currentTimeMillis - time), 0))\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
